---
title: "CMSC 320 - Final Project"
author: "Rakha Wibisana"
date: "May 20, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

##Data Science: A Tutorial  
  
Data science is a hot topic in today's technologically-driven society, especially with the amount of data that gets accrued every single day. But what exactly is data science and how can it help us? _Data science_ is the process of creating insights in data that help drive decisions or answer questions. These insights can help financial analysts predict market trends, advise doctors where new strains of a virus may pop up, and much more.  
  
While data science can take the shape of many forms in a plethora of disciplines, the process of turning data into knowledge can be abstracted into a series of steps known as the data science pipeline. The data science pipeline consists of: preparing the data; performing an exploratory data analysis; conducting a series of hypothesis tests and data analysis methods on the data; and finally, taking the insights that you've gathered from the data and synthesize in a way that other people can make inferences from it.  
  
In this tutorial, I will take you across all of these steps of the data science pipeline using a dataset of listings obtained from Airbnb in the San Francisco, CA area. The data can be found at http://insideairbnb.com/get-the-data.html.  
  
###About the Dataset  
  
I chose to perform an analysis on Airbnb data in San Franscisco, CA because I knew a number of friends who were looking for places in the area to live over the summer for their summer internships. Listening to their conversations, it was apparent that finding a suitable listing for them was difficult because either the prices were too high or the location was too inconvenient for them. Therefore, I decided to analyze the Airbnb data to see if I could make sense of what could result in the variation of pricing between listings.  
  
Furthermore, I wanted to create a model that could predict what a listing's average price would be given a certain set of conditions. Being able to determine if a listing is priced fairly is important because it could prevent interns like my friends from getting unfairly charged for their accomodations. Price gouging, or the act of raising prices beyond a normal level to exploit customers, is not unheard of in the housing industry. In 2018, [Airbnb was criticized for allowing hosts to list their rooms for exorbitant prices during the 2018 Super Bowl](http://insideairbnb.com/get-the-data.html). According to the linked article, a two-bedroom apartment in downtown Minneapolis was reported to have changed their listing price from the normal price of $75 per night to an incredible **\$5,000 per night** during the Super Bowl.  
  
###Preparing the Data  
  
Preparing the data is arguable one of the most time-intensive processes within the data science pipeline. Often, the data that you obtain will come in a way that's not readily usable and must require some modifications. Before we can prepare the data, we must obtain the data. One method of obtaining data involves scraping the internet for usable data. Although I did not use this technique to get my dataset, it's still an important skill to learn as most data will not come in such a nice format. [This article does a good job at taking a person through the process of scraping data from the web](https://medium.freecodecamp.org/an-introduction-to-web-scraping-using-r-40284110c848).  
  
If the data is already compiled into a dataset within a file, you can simply load the data into the R environment. Data can come in many filetypes, including .csv, .xlsx, and more. It's important to know how to properly load in the data according to the filetype. The dataset I obtained from Airbnb is in a .csv format, so I'm going to use the _read\_csv()_ function within the **tidyverse** package to read in the file. Other file formats might also require the help of an additional library to load the data. [Take a look at this website to see how to load in other filetypes to R](https://www.statmethods.net/input/importingdata.html).  
  
```{r data_cleaning1}
library(tidyverse)
# load in the csv file. col_names is set to T because my data has headers that should be separated from the observations in the data
data <- read_csv("data/listings.csv", col_names=T)
```  
  
Once the dataset has been loaded in, we can start to prepare the data. This includes finding the data and manipulating it in a way that allows it to be analyzed more easily. Many of the methods used in data science to analyze a dataset assume that the data is formatted in a tidy way. Data is _tidy_ when:  
1. **Each row in the data table corresponds to a single entity or relationship.** In other words, each row should only contain information regarding one object, such as a single listing for Airbnb and not more.  
2. **Each column in a data table corresponds to single attribute or variable that describes each entity.** An example of when you would separate a column into multiple columns is if a column contained information about both gender and age, encoded as 'm24' denoting a male who was 24 years old. This should be separated into two columns - one for gender and another for age.  
3. **Each data table corresponds to a single entity set or relationship.** For example, a data table containing information about listings and personal customer information would better be separated into two data tables - one for listings and another for the customer information.  
  
For more information about what constitutes tidy data, as well as how to make data tidy, you can [check out this tutorial](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) that goes into detail about this topic.  
  
The dataset that I'm working with is very tidy as is, so I don't have to worry too much about modifying it too much. However, let's take a look at just how many attributes are contained within the dataset itself. A good way to check the dimensions of the data is through the function _dim()_.  
  
```{r data_cleaning2}
# checking dimensions of the data
dim(data)
```  
  
Since tables are read in row by column format, the dimensions for the dataset are returned respectively. This output means that the dataset has **7353 listings** and each listing has **106 attributes**. For our actual analysis of the data, 106 attributes is far too many to look at! One of the goals of data science is create the most parsimonious, or simple, model for our data. One way to achieve this is by limiting the amount of attributes included. In order to do this, I will use the _select()_ function within the _tidyverse_ package from before. For future reference, the [tidyverse package](https://www.tidyverse.org/) is one of the most useful packages in data science within R because it provides many functions and shortcuts for processing the data.  
  
One thing to note about selecting attributes is that you ideally want to select attributes that most observations within your dataset have actual values for, and don't have missing values. Handling missing values is typically a part of exploratory data analysis, but we can also handle missing values during the preparation phase. Missing values in R are denoted with an _N/A_ and should be handled during the data preparation phase since the methods we use to analyze the data can't handle missing values. One way to check how many observations actually have values for a given attribute is through the _is.na()_ function. You can access a vector of an attribute using the '$' operator. Here's an example of how you would check for missing values:  
  
```{r data_cleaning3}
# check how many observations have missing values for the square_feet attribute
sum(is.na(data$square_feet))
```  
  
As you can see, 7227 of the 7353 entities have missing values for the square_feet attribute. Therefore, we shouldn't include this attribute within our dataset for analysis. If an attribute has some entities that are missing values, you can either choose to delete those observations with missing values, or change their values to work with the dataset. [Here's a good article on how to handle missing data in R](https://medium.com/coinmonks/dealing-with-missing-data-using-r-3ae428da2d17).  
  
Selecting attributes can be a tricky process, because although you want to minimize the amount of variables in your model, you also don't want to leave out any potentially important variables that could help explain the data better - also known as omitted variable bias. Having domain knowledge in the subject of inquiry can help a lot in trying to select the attributes in a model. Using my limited knowledge of what factors might influence a listing's price, I will select the attributes in the following code snippet. I will also remove any entities that have _N/A_ in any columns from the dataset. 
  
```{r data_cleaning4}
# create a column vector of selected attributes using c()
selected_cols <- c("host_response_time","host_response_rate","host_is_superhost","host_listings_count","host_has_profile_pic","host_identity_verified","neighbourhood_cleansed","property_type", "room_type", "accommodates","bathrooms","bedrooms","beds","bed_type","amenities", "price", "security_deposit","cleaning_fee","guests_included","extra_people","minimum_nights","maximum_nights","availability_30","availability_60","availability_90","availability_365","number_of_reviews","review_scores_rating","review_scores_accuracy","review_scores_cleanliness","review_scores_checkin","review_scores_communication","review_scores_location","review_scores_value","cancellation_policy","reviews_per_month")

# refine the original dataset to only contain the selected attributes
data_min <- data %>%
  select(selected_cols)

# replace "N/A" values with null
data_min <- na_if(data_min, "N/A")

# remove any entities with N/A or null
data_min2 <- na.omit(data_min)

# check how many observations remain
nrow(data_min2)
```  
  
Often times, the types of the attributes you select might not be the correct types needed for analysis. One instance of an incorrect type is when the values are simply encoded as a different representation of itself. In my dataset, the "host_response_rate" attribute has values that are encoded as chars, when I want them as numerics. I can use a combination of string parsing and type conversion to convert the type into what I need it to be. Another case of an incorrect type is when a variable that is categorical in nature isn't encoded as such. In my dataset, the "host_is_superhost" attribute should be a binary variable where the value is equal to 1 when the host is a superhost, instead of having char values of 't' and 'f'. This conversion can be done through the _factor()_ function.  
  
There may even be instances where you have to split an attribute into multiple columns in order to maintain a tidy dataset where only one attribute is included within a column. In my dataset, "amenities" is a list of amenities that the host provides for. In order to tidy the data, I will split "amenities" into a series of boolean attributes where the value is 1 if the host provides it (e.g. has_internet, has_wifi, etc.). For the purposes of this tutorial, I will only create variables for TV, Internet, Wifi, Kitchen, Heating, Air Conditioning, Dryer, and Washer.  
  
```{r data_cleaning5}
# parse amenities into separate boolean attributes and removes amenities
data_min3 <- data_min2 %>%
  mutate(has_TV = str_detect(data_min2$amenities, "TV"), has_internet = str_detect(data_min2$amenities, "Internet"), has_wifi = str_detect(data_min2$amenities, "Wifi"), has_kitchen = str_detect(data_min2$amenities, "Kitchen"), has_heating = str_detect(data_min2$amenities, "Heating"), has_ac = str_detect(data_min2$amenities, "Air conditioning"), has_dryer = str_detect(data_min2$amenities, "Dryer"), has_washer = str_detect(data_min2$amenities, "Washer")) %>%
  select(-c("amenities"))

# convert host_response_rate from percentage char to decimal (numeric)
data_min3 <- data_min3 %>%
  mutate(host_response_rate = as.numeric(gsub("%", "", data_min3$host_response_rate))/100)

# convert price from a currency char to a decimal (numeric)
data_min3 <- data_min3 %>%
  mutate(price = as.numeric(gsub("[$,]", "", data_min3$price)))

# convert security deposit from a currency char to a decimal (numeric)
data_min3 <- data_min3 %>%
  mutate(security_deposit = as.numeric(gsub("[$,]", "", data_min3$security_deposit)))

# convert cleaning fee from a currency char to a decimal (numeric)
data_min3 <- data_min3 %>%
  mutate(cleaning_fee = as.numeric(gsub("[$,]", "", data_min3$cleaning_fee)))

# convert extra people from a currency char to a decimal (numeric)
data_min3 <- data_min3 %>%
  mutate(extra_people = as.numeric(gsub("[$,]", "", data_min3$extra_people)))

# select the columns to convert to factors
factor_columns <- c("host_response_time","host_is_superhost","host_has_profile_pic","host_identity_verified","neighbourhood_cleansed","property_type", "room_type","bed_type", "cancellation_policy")

# factors the categorical variables specified by factor_columns
data_min3[, factor_columns] <- lapply(data_min3[, factor_columns], as.factor)
```  
  
Now that the data has been prepared, we can continue to performing an exploratory data analysis.  
  
###Exploratory Data Analysis  
  
Often in data science, we want to perform an exploratory data analysis (EDA) of the data to get a better understanding of it. EDA is also necessary to see if certain assumptions are held up when performing data analysis procedures. For example, later on when we create a linear regression model of the data with price as the response variable, we want to check to make sure the residuals comply by the assumptions of linear regression. EDA encompasses visualizing the data, performing summary statistics, transforming the data if necessary, and handling missing data. We covered how to handle missing data in the previous section, so in this section we will cover the other three areas.  
  
####Visualizing the Data  
  
Visualizing data can be accomplished by plotting the data points of interest on a plot and examining it for any visible patterns. Plotting data in R is made easy through the _ggplot2_ package. The package is also included within _tidyverse_.[This resource is a good reference for learning how to use ggplot2 in R to visualize data](https://datacarpentry.org/R-ecology-lesson/04-visualization-ggplot2.html).  
  
To give an example of how we can visualize data in R, we will plot the response variable of interest in our tutorial, price. To observe any patterns in price, we will analyze its histogram.  
  
```{r eda1}
# Plotting the histogram for price using ggplot2
data_min3 %>%
  ggplot(aes(price)) +
  geom_histogram()
```  
  
####Summary Statistics  
  
When performing a visual analysis of the data, try and determine the shape, outliers, center, and spread - also known as SOCS! These are what make up the summary statistics of data.  
  
The first thing we can observe from this histogram is its **shape**. Looking at the histogram, we can determine that it is skewed right, as shown by the long tail of values on the right side. We can also see that the distribution is unimodal, meaning that it has one mode, or maximum. [This website offers a brief, yet helpful explanation of how to observe the shape of a distribution](https://mathbitsnotebook.com/Algebra1/StatisticsData/STShapes.html).  
  
The next visual observation that we can identify is if there are any **outliers**. Outliers are points that stray far away from the main distribution. In our case, there are outliers towards the upper end of the price range all the way up to $8000.  
  
The third characteristic we observe is the **center** of the distribution. The two main measures of spread are mean and median. The mean of the distribution is the best estimate for center since it is the average of all values. However, if the distribution is skewed like the distribution for price is, then the mean becomes skewed in the direction of the tail. Therefore, in skewed distributions, median is a better measure of center. [A recap of central tendency can be found on this website](http://condor.depaul.edu/sjost/it223/documents/central.htm).  
  
The fourth observation we can see is the distribution's **spread**. Spread is the measure of variability in the data. One measure of spread is range, and in the price distribution, we can see that the range of values go from \$0 all the way up to \$8000. Looking at just the distribution can be hard to estimate the exact range, which is why we then observe the five-number summary of the data.  
  
```{r eda2}
# get summary statistics for price
summary(data_min3$price)
```  
  
From the five-number summary, we can see the minimum, maximum, first quartile, median, and third quartile of the data. We can see that although the range is 8000, the median of the prices is $150, meaning that this distribution is heavily skewed to the right. However, this type of pattern in pricing is not abnormal, as a lot of financial measures in the real world have a majority of their values on the "lower" end, while having less values on the "high" end. In order to accomodate for skewing, we can perform transformations on variables such as price.  
  
####Transforming the Data  
  
Transforming data in R is useful because it can help change data in a way that allows it to be used in certain data analysis methods. One example where data transformations are necessary is evident in machine learning when creating a neural network. Data transformations in the form of standardizing the data are necessary for a neural network to be created due to its use of linear combinations between nodes. In our case, we can use transformations in the form of a logarithmic transform to unskew the price attribute.  
  
```{r eda3}
# log transform of price
data_min4 <- data_min3 %>%
  mutate(log.price = log(price)) %>%
  select(-c("price"))

# remove -Inf log.price value since there is only one instance
sum(data_min4$log.price == -Inf)
data_min4 <- data_min4 %>%
  filter(log.price != -Inf)

# plot the histogram of price
data_min4 %>%
  ggplot(aes(log.price)) +
  geom_histogram()
```  
  
As we look at the histogram of log.price, we can see that the data is more normally distributed now. Be cautious about transformations though, as [it can affect the way you interpret the data](https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/). [Check out this website for more information on how to perform transformations in R](http://rcompanion.org/handbook/I_12.html).  
  
###Hypothesis Tests and Data Analysis Methods  
  
[Hypothesis tests](https://stattrek.com/hypothesis-test/hypothesis-testing.aspx) are an essential part of data science because they tell us whether or not the results we observe are statistically significant. In a hypothesis test, we have the null hypothesis, which we assume to be true, and the alternative hypothesis. Based on the significance level (which is usually $\alpha$ = 0.05 unless otherwise stated), we can look at the p-value of a test statistic to see if the results observed are in fact statistically significant. The p-value is the probability that the results of the distribution are greater than or equal to the observed results, assuming the null hypothesis is true. If the p-value is less than the significance level, then we reject the null hypothesis in favor of the alternative hypothesis; otherwise, we fail to reject the null hypothesis.  
  
Many hypothesis tests have a null hypothesis that assumes that there is "no difference" between two models that are being compared. For example, when we create a linear model of the relationship between price and the other predictors in the dataset, the resulting summary table gives us the p-values of each attribute and its effect on price.  
  
```{r hyp_test1}
# create linear regression model of other predictors on log.price
linear_model <- lm(log.price~., data=data_min4)
# summary of linear model fit
summary(linear_model)
```  
  
We can look at the p-value of each attribute to see if that attribute is significant in predicting price, holding all other predictors constant. For example, if we look at the p-value of maximum_nights, we see that it is 0.575 which is greater than our significance level of 0.05. Therefore, we would consider removing it from the model as its effects on price aren't statistically significant, meaning that a change in the maximum amount of nights that a person can stay doesn't affect price.  
  
On the other hand, if we look at the p-value for minimum_nights, we see that its p-value is < 2e-16, which is much less than 0.05. Therefore, we would keep minimum_nights in the linear model as it has a statistically significant effect on price, meaning that a change in the minimum amount of nights that a person can stay does in fact affect the price of the listing.  
  
[Linear regression](https://www.geeksforgeeks.org/ml-linear-regression/) is a supervised machine learning method, meaning that it predicts a response variable given a set of predictors. One of the pitfalls of linear regression is that the inclusion of insignificant predictors can greatly weaken the overall predicting power of the model. In this case, hypothesis tests can help us select the variables that go into the model. We can remove the insignificant predictors at a 5% level from our model. Once we obtain the reduced model, we can perform an [ANOVA test](https://bookdown.org/ndphillips/YaRrr/comparing-regression-models-with-anova.html) to see if the reduced model is in fact a better fit for the data. The null hypothesis of the ANOVA test claims that the complex model is a worse or equal fit to the data than the reduced model. Therefore, if the p-value of the F-statistic from the ANOVA table is less than the significance level of 0.05, then we choose the more complex (full) model.  
  
```{r hyptest2}
# select attributes that aren't significant or have at least one significant level in their factor
insignificant_predictors <- c("host_response_time", "host_response_rate", "host_has_profile_pic", "bathrooms", "bed_type", "security_deposit", "cleaning_fee", "extra_people", "maximum_nights", "review_scores_accuracy", "review_scores_communication", "has_internet", "has_kitchen", "has_heating", "has_dryer", "has_washer")

# remove insignificant predictors
data_reduced <- data_min4 %>%
  select(-c(insignificant_predictors))

# create new reduced linear model
lm_reduced <- lm(log.price~., data=data_reduced)

# perform anova test with reduced model and full model
anova(lm_reduced, linear_model)
```  
  
As we can see from the results of the ANOVA table, the p-value of the F-statistic is 0.015 which is less than 0.05. Therefore, we reject the null hypothesis and choose the more complex (full) model. In this case, removing the insignificant predictors from the model didn't make the model better overall at predicting log.price.  
  
Before we accept the linear regression model, we have to check the residuals of the linear model to make sure that they meet the [assumptions of linearity](http://r-statistics.co/Assumptions-of-Linear-Regression.html). Using _plot()_ on the linear model and _hist_ on the residuals of the linear model can let us see if the assumptions are met. The four assumptions of a linear regression model are **l**inearity, **i**ndependence, **n**ormality, and **e**qual spread of the residuals.    
  
```{r hyptest3}
# plot the graphs for the linear model checking the residuals
plot(linear_model)

# check the histogram of residuals to see if it's normally distributed
hist(linear_model$residuals)
```  
  
* Linearity: the residuals vs. fitted plot has randomly scattered residuals centered around the mean = 0. Therefore, this assumption is checked.
* Independence: the Scale-Location plot shows no visible pattern in the fitted values plot. Therefore, independence is checked.
* Normality: the histogram of the residuals has a roughly normal distribution. Therefore, normality is checked.
* Equal Spread: the residuals vs. fitted plot has equal variance throughout the fitted values and there is no apparent funnel shape in the residuals. Therefore, equal spread is checked.  
  
All the assumptions for the linear regression model are met, so this is a valid linear regression model.  
  
###Insights  
  
The last step of the data science pipeline is to create insights from the data and the models that describe it. In this tutorial, we walked through how to create a model using linear regression. Linear regression is great because it is relatively more interpretable than other machine learning methods such as machine learning, which is a black box meaning that the way it actually works is hard to interpret.  
  
On the other hand, in linear regression, we can see exactly which predictors are the most significant in predicting the response by looking at which ones have the smallest p-values. For example, in our model a significant predictor was bedrooms which had an extremely small p-value of < 2e-16. This makes sense, as the number of bedrooms that a listing has would likely increase the price of the listing as that number went up.  
  
Furthermore, by looking at the coefficients of each predictor, we can see if an increase in that predictor will cause an increase or decrease in the response. Going back to the bedroom attribute, it had a coefficient of 1.772e-01. Since the coefficient is positive, we can determine that as the number of bedrooms increases, so does the price of the listing. However, since we had to perform a log transformation on price in our model, the exact interpretation of the relationship between the attribute and the response becomes a little tricky. Since we took the log of price, price no longer changes by the coefficient of the predictor, but rather by (exp(predictor) - 1)\*100 percent. For example, for each one unit increase of bedrooms, the price of the listing increases by (exp(1.772e-01) - 1)\*100 percent on average, while holding all other predictors fixed.  
  
Combining both p-values and coefficients, we can see which neighborhoods in this dataset have more expensive listings on average. For example, the Castro/Upper Market neighborhood has a p-value < 2e-16 and a coefficient of 4.592e-01. This means that listings in the Castro/Upper Market neighborhood are more expensive than the baseline neighborhood, Bayview. With this information, a student interning in San Francisco, CA can choose to target Airbnb listings in areas that are more affordable to them.  
  
###Conclusion  
  
I hope that this tutorial did well to explain the basics of how to utilize the data science pipeline. In this tutorial, we covered how to collect and clean data for analysis; how to perform an exploratory data analysis of an attribute; perform hypothesis tests and create a simple supervised machine learning model through linear regression; and extract valuable insights from the model that satisfied our initial goal of trying to understand what variables had the most influence on a listing's price. Thank you for reading through my tutorial, and I hope that you were able to learn a little (or a lot) about data science along the way!